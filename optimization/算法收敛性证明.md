---
export_on_save:
html: true
---


<!-- TOC -->

1. [常见的约束条件](#常见的约束条件)
2. [GLM](#glm)
3. [General Gradient Descent(略)](#general-gradient-descent略)
4. [GD for$\beta-$smooth function](#gd-forbeta-smooth-function)
5. [$\alpha$-strong convex function](#alpha-strong-convex-function)
6. [Projected GD](#projected-gd)
7. [proximal gradient descent](#proximal-gradient-descent)
8. [subgradient](#subgradient)
9. [$ G_\gamma $ function](#-g_gamma--function)
10. [Acclerated GD](#acclerated-gd)
11. [ADMM(Alternateive Direction Method of Multiplies)](#admmalternateive-direction-method-of-multiplies)

<!-- /TOC -->
### 常见的约束条件
- $ \Omega _1 =  ${nonnegative}
- $ \Omega _2= ${Box}
- $ \Omega _3= \{l_2-ball\}$
- $ \Omega _4= ${Affine}
以上适合用projceted GD,下面的适合用proximal GD
- $ \Omega _5= \left\{ x|x \geq0,1^Tx=1 \right\}  $
- $ \Omega _6= \left\{ x| \left\| x \right\|^2  \right\} \leq s $
- $ \Omega _7= \left\| x \right\|_1 \leq s  $
- $ \Omega _8= \left\{ x|\left\| x \right\| _q \leq 1 ,0<q<1\right\}  $
### GLM
首先先给出GLM的模型推导，因为这个模型后面经常用。

> Dataset is$\{(a_i,b_i)\}^N,a_i\in R^p,b_i\in R,f:R^p\rightarrow R$,logistic regression:$min_x\{\sum_{i=1}^Nlog(1+exp(a_i^Tx))-b^TAx\}$

上面这个模型是怎么样得到的，因为我们有假设$log(\frac{p_i}{1-p_i})=a_i^Tx\Rightarrow p_i = \frac{1}{1+exp(-a_i^T)}\Rightarrow 1-p_i=\frac{1}{1+exp(a_i^Tx)}$

logistic模型本质是一个似然估计思想， 将其看成一个Bernoulli distribution 
$$f(bi|p_i)=p_i^{b_i}(1-p_i)^{1-b_i}\Rightarrow log(f)=-log(1+exp(a_i^Tx))+b_ia_i^Tx$$

$$\Rightarrow \Pi_{i=1}^Nlog(b_i|p_i)=\Sigma_{i=1}^{N}(-log(1+exp(a_i^Tx))+b_ia_i^Tx)$$
对于上式取极大，即得到取极小的定理$min_x\{\sum_{i=1}^Nlog(1+exp(a_i^Tx))-b^TAx\}$


### General Gradient Descent(略)

### GD for$\beta-$smooth function
> Def: a function f is called$\beta-$smooth function, if$\forall x,y \in dom(f),||\nabla{f(x)}- \nabla{f(y)}||^2\leq\beta ||x-y||^2,\beta>0$

- ex1:$f(x)=b^TAx\Rightarrow \nabla{f(x)}=A^Tb,\forall x,y, ||\nabla{f(x)}-\nabla{f(y)}||=0$,因此其$\beta=0$为0-smooth function.
- ex2:$f(x)=\frac{1}{2}||Ax||^2,\nabla{f(x)}=A^TAx\Rightarrow ||A^TA(x-y)||\leq \lambda_{max}{A^TA} \left\| x-y \right\|$,因此其为一个$\lambda_{max}(A^TA)-$smooth function(where$\lambda_{max}(A^TA)$表示是矩阵$A^TA$的最大特征值).
> lemma1:$\forall x,y \in dom(f)$$f is$$\beta$-smooth function$f(y) \leq f(x)+(\nabla{f(x)},y-x)+\frac{\beta}{2} \left\| x-y \right\|^2$

pf:
$$g(t) =f(x+t(y-x)),g(0)=f(x),g(1)=f(y)$$
$$\therefore f(y)-f(x)=g(1)-g(0)=\int_0^1g^{\prime}(t)dt=\int_0^1(\nabla{f(x+t(y-x)),(y-x)})dt$$
$$f(y)-f(x)-(\nabla{f(x)},y-x)=\int_0^1(\nabla{f(x+t(y-x))-\nabla{f(x)}},y-x)dt$$
$$\overset{CS-inequality}{\leq}\int_0^1 \left\| \nabla{f(x+t(y-x))-\nabla{f(x)}} \right\| \left\| y-x \right\| \det $$
$$\leq\int_0^1t\beta \left\| y-x \right\| ^2dt=\frac{\beta}{2}\left\| y-x \right\|^2 \blacksquare $$

> - denote：$M_t(x)= f(x_t)+(\nabla{f(x_t)},x-x_t)+\frac{\beta}{2} \left\| x_t - x\right\|^2$
> - from lemma, we know$f(x)\leq{m_t(x)}$
> -$\nabla{m_t(x)}=\beta(x_t-x)+\nabla{f(x_t)} \Rightarrow x_{t+1} = x_t - \frac{1}{\beta}\nabla{f(x_t)}$(根据凸函数的非负加权合仍为凸函数,所以导数为0的地方，其达到值最小)
> -$\Rightarrow f(x_{t+1})\leq m_t(x_{t+1})\leq m_t(x_t) = f(x_t)             \qquad                                  (1)$

上面的证明回答了**为什么对于$\beta$-smooth function$\frac{1}{\beta}$是一个较好的步长选择**

因此对于linear regression 来说

$$min{\frac{1}{2}\sum_i^N{(a_i^Tx)}-b^TAx}\Rightarrow \beta = \lambda _{max}(A^TA)$$

$$\Rightarrow x_{t+1} = x_t - 1/\lambda _{max}(A^TA)\nabla{L(x_t)}$$

> Theorem1: Suppose that f is a$\beta$- smooth function$x^{t+1} = x^{t} - \frac{1}{\beta}\nabla{f(x^t)}$then 
$$min_t \left\| \nabla{f(x_t)}  \right\| \leq \sqrt{\frac{2\beta f(x_0)}{T}}\overset{T}{\rightarrow} 0$$
> Theorem2: f is a convex and$\beta$-smooth function,$x_{t+1} = x_{t}-\frac{1}{\beta}\nabla{f(x_t)}$, then
$$f(x_T) - f(x^*)\leq \frac{\beta}{2T} \left\| x^T-x^* \right\|^2 $$

pf:

$$M_t(x)= f(x_t)+(\nabla{f(x_t)},x-x_t)+\frac{\beta}{2} \left\| x_t - x\right\|^2$$

$$\nabla{m_t(x)}=\nabla{f(x_t)}+\beta(x-x_t)$$

$$\nabla^2{m_t(x)} = \beta I \geq \beta I$$
因此$m_t(x)$为一个$\beta$-strong convex function
$$\Rightarrow m_t(y) \geq m_t(x) + (\nabla{m_t(x)},y-x)+\frac{\beta}{2}\left\| x_t-x \right\| ^2$$
denote$y=x^*,x=x^{t+1}$
$$m_t(x^*)\geq m_t(x_{t+1})+ (\nabla{m_t(x_{t+1})},x^*-x_{t+1})+\frac{\beta}{2}\left\| x^*-x_{t+1} \right\| ^2$$

$$\geq m_t(x_{t+1})+\frac{\beta}{2}\left\| x^*-x_{t+1} \right\| ^2(因为x_t+1正好是使其梯度为0的点)$$


由$m_t(x)$的性质(1)可以知道
$$
f(x_{t+1})\leq m_t(x_{t+1})\leq (m_t(x^*)=m_t(x_t))-\frac{\beta}{2} \left\| x^*-x_{t+1} \right\|
$$
$$\overset{m_{t}(x)的定义}{\leq}f(x_t)+(\nabla{f(x_t)},x^*-x)+\frac{\beta}{2}\{ \left\| x^*-x_t \right\|^2-\left\| x^*-x_{t+1} \right\|^2  \}$$

$$\overset{凸性}{\leq } f(x^{*})+\frac{\beta}{2}\{ \left\| x^*-x_t \right\|^2-\left\| x^*-x_{t+1} \right\|^2  \}\\\therefore f(x_{t+1})-f^*\leq \frac{\beta}{2}\{ \left\| x^*-x_t \right\|^2-\left\| x^*-x_{t+1} \right\|^2  \})\\\therefore f(x_{t})-f^*\leq \frac{\beta}{2}\{ \left\| x^*-x_{t-1} \right\|^2-\left\| x^*-x_{t} \right\|^2  \})\\\vdots\\\sum_{t=0}^Tf(x_t)-Tf(x_0)\leq \frac{\beta}{2}\{ \left\| x^*-x_{t-1} \right\|^2-\left\| x^*-x_{t} \right\|^2\}) \blacksquare  $$
之后再两边同时除以T就得证了。

### $\alpha$-strong convex function
> Def:$\alpha$-strong convex function iff 
$$\forall x,y \in dom(f)\\
f(y)\geq f(x)+(\nabla{f(x)},y-x)+\frac{\alpha}{2} \left\| y-x \right\|^2$$

> lemma2: f(x) is$\alpha$-strong convex function$\iff$$f(x) - \frac{\alpha}{2}\left\| x \right\|^2$is convex


> Theorem1:$f \in C^1$[continuous and differentiable],then the following arguments are equalient:
> - f is$\alpha$- strong convex
> -$(\nabla{f(y)}-\nabla{f(x)},y-x)\geq \alpha \left\| x-y \right\|^2$
>  -$f\in C^2,\nabla^2{f(x)}\geq \alpha I$

> Theorem1: Assume that f is$\alpha$- strong convex and$\beta$-smooth function
$$x_{t+1} = x_t - \frac{1}{\beta}\nabla{f(x_t)}$$
then$\left\| x_T-x^* \right\|  \leq exp(-\frac{T}{\tau } ) \left\| x^0-x^* \right\|^2$where$\tau  = \frac{\lambda_{max}(A^TA)}{\lambda_{min}(A^TA)}$

pf:
$$\left\| x_{t+1}-x^* \right\|^2 = \left\| x_t - \frac{1}{ \beta } \nabla{f(x_t)} - x^*  \right\|^2 \\ = \left\| x_t - x^* \right\|^2 + \frac{1}{\beta ^2} \left\| \nabla{f(x_t)} \right\|^2 - \frac{2}{ \beta } ( \nabla{f(x_t)},x_t - x^* ) \tag{\textcircled{1}}$$
$$f(x^*) \geq f(x_t)+( \nabla{f(x_t)},x^*-x_t ) + \frac{\alpha}{2} \left\| x^*-x_t \right\|^2 $$
$$\therefore ( \nabla{f(x_t)}, x^*-x_t ) \leq f(x^*) - f(x_t) - \frac{\alpha}{2} \left\| x^*-x_t \right\|^2 
\\
\therefore \text{\textcircled{1}} \leq \left\| x_t - x^* \right\|^2 + \frac{1}{\beta ^2} \left\| \nabla{f(x_t)} \right\|^2 + \frac{2}{\beta }(f(x^*)-f(x_t) - \frac{\alpha }{2}\left\| x^*-x_t \right\|^2 )\\
\leq (1-\frac{\alpha}{\beta })\left\| x_t-x^* \right\|^2 - \frac{2}{\beta }(f(x_t)-\frac{1}{2 \beta }\left\| \nabla{f(x_t)} \right\|^2 - f^* )
$$

denote$m_t(x) \overset{\Delta}{=}f(x_t)+(\nabla{f(x_t)},x-x_t)+\frac{\beta}{2} \left\| x-x_t \right\|^2 and$
$$x_{t+1} = argmin_x(m_t(x))=-\frac{1}{\beta }\nabla{f(x_t)}+x_t$$

$$
\therefore m_t(x_{t+1}) = f(x_t)+( \nabla{f(x_t)},-\frac{1}{\beta } \nabla{f(x_t)} )+ \frac{\beta}{2} \left\| -\frac{1}{\beta } \nabla{f(x_t)} \right\|^2 \\ = f(x_t) - \frac{1}{2 \beta} \left\|  \nabla{f(x_t)}  \right\|^2 
$$
we know$f^* \leq f(x_{t+1}) \leq m_t(x_{t+1}) \\
\Rightarrow (f(x_t)-\frac{1}{2 \beta }\left\| \nabla{f(x_t)} \right\|^2 - f^* )>0$
$$\left\| x_{t+1} - x^* \right\|^2 \overset{\text{由最开始的式子}}{\leq} (1-\frac{\alpha }{\beta }) \left\| x_t - x^* \right\|^2$$
递推得到$\left\| x_T - x^* \right\|^2 \leq (1-\frac{\alpha}{\beta})^{t+1} \left\| x_0-x^* \right\|^2$
$$\overset{by (1-x)^t \leq exp(-xt)(0<x<1)}{\Rightarrow} 证毕\blacksquare $$


### Projected GD
some constraints
- non-convex$\Omega  = \{x| \left\| x \right\|_q \leq R \} ( 0<q<1 )$where$\left\| x \right\|_q = (\sum \left| x_i \right|^q )^{\frac{1}{q}}$
- group sparse constrains:$\Omega  = \{ x | \left\| x \right\|^2 \leq R \}$

> Theorem1: 
>$$min \quad  f(x)\newline
s.t. x \in \Omega 
>$$
> step1: given$x_0$,t = 0
> step2:$y_{t+1} = x_t + \gamma _th_t,h_t = -\nabla{f(x_t)} \\
> \qquad x_t+1 = \Pi _{\Omega}(y_t) \in \Omega$

> Def : the projection of a point y on the set C is 
>$$\Pi _{\Omega}(y) = argmin_{x\in C} \left\| x-y \right\|$$

> ex1:
> nonnegative 
>$\Omega  = \{ x|x\geq 0\} \Pi _{\Omega }(y) = max(y,0)$
> ex2:
> l_2-ball
>$\Omega = \{ x| \left\| x \right\|^2 \leq 1\}$
$$\prod_{\Omega}(y)=\left\{\begin{array}{l}
y,\left\| y \right\|^2 \leq 1  \\
\frac{y}{\left\| y \right\|^2 },otherwise
\end{array}\right.$$
ex3:
affine constrain
$\Omega  = \{ A_{n\times p}x_{p\times 1} = b_{n\times 1}\}$
$y^{ \perp } = y - \Pi _{\Omega }(y)=y-Ax^*$
$(y^{ \perp },Ax) = 0 (for \forall x)\Rightarrow (y-Ax^*)^TA = 0 \\
\Rightarrow A^TAx^*=A^Ty$
if$A^TA$is invertable$x^* = (A^TA)^{-1}A^T y$

> example: min$\frac{1}{2} \left\| Ax-b \right\|^2 s.t.x \geq 0 \\
> x_{t+1} = max(x_t - \frac{1}{\beta }(Ax_t-b),0)(where \beta = \lambda _{max}(A^TA))$

> Theorem1: f is convex, differentiable, we denote an algorithms 
>$$\left\{\begin{array}{l}y_{t+1} = x_t - \gamma \nabla{f(x_t)}(\gamma =\frac{R}{L\sqrt{T}})\\ x_{t+1} = \Pi _{\Omega }(y_{t+1})\end{array}
\right.$$
$\left\{ x_t \right\} _{t=1}^T$if we suppose$\left\| \nabla{f(x_t)} \right\|^2 \leq L ,\left\| x_1-x^*\right\|  = R$
then$f(\frac{1}{T}\sum_{t=1}^{T}x_t)-f^* \leq \frac{RL}{ \sqrt{T} }$

pf:
convex function so we have$f(x_t) - f^* \leq ( \nabla{f(x_t)},x_t-x^*) = \frac{1}{\gamma }(x_t-y_{t+1},x_t-x^*)$
by$\left\| a \right\|^2 +\left\| b \right\|^2  = \left\| a-b \right\|^2 +2(a,b)$
上式=
$$\frac{1}{2\gamma }(\left\| x_t-y_{t+1} \right\|^2 + \left\| x_t-x^* \right\|^2 - \left\| x^*-y_{t+1} \right\|^2 )$$
$$=\frac{1}{2\gamma}(\left\| x_t-x^* \right\|^2 - \left\| x^*-y_{t+1} \right\|^2 ) + \frac{\gamma }{2}  \left\| \nabla{f(x_t)} \right\|^2$$
by$\left\| x^*-y_{t+1} \right\|^2 \geq \left\| x^*-x_{t+1} \right\|^2$(由于$x^*$在$\Omega$空间内故距离在减小)
故上式
$$\leq \frac{1}{2 \gamma }(\left\| x_t-x^* \right\|^2 - \left\| x^*-x_{t+1} \right\|^2 + \frac{L^2 \gamma}{2})$$
$$\therefore f(x_1) \leq \frac{1}{2 \gamma }(\left\| x_1-x^* \right\|^2 - \left\| x_2-x^* \right\|^2 +\frac{L^2 \gamma}{2})\\ \vdots\\f(x_T)-f^* \leq \frac{1}{2\gamma }(\left\| x_T - x^* \right\|^2 + \left\| x_{t+1}-x^* \right\|^2 +\frac{L^2 \gamma }{2})$$
累加即可得到
$$\sum_{t=1}^{T}f(x_t)-Tf^* \leq \frac{1}{2 \gamma}(\left\| x_1-x^* \right\|^2 -\left\| x_{T+1}-x^* \right\|^2 )+\frac{L^2\gamma T}{2}\\\Rightarrow f(\frac{1}{T}\sum_{t=1}^{T}x_t)-f^* \leq \frac{1}{T}\sum_{t=1}^{T}f(x_t) - f^* \leq \frac{R}{2 \gamma T}+\frac{L^2 \gamma}{2} \overset{C-S}{\leq}\ldots \blacksquare  $$

### proximal gradient descent 
在考虑proximal gradient descent 前，需要先考虑如何将 constrainted optimization covert to **non-constrainted optimization**
by using indicator function, we have:
$$ 
min_{x \in R^d}f(x)+\delta _{\Omega}(x) 
$$
**indicator function**
$$ 
\delta_{\Omega}(x) = \left\{\begin{array}{l}
    0, \qquad if  \quad x \in \Omega \\ +\infty ,\quad if \quad x \not\in \Omega 
\end{array}
\right.
$$
所以在constraint里面在的x可以认为其就是在做原始的优化问题，不在constraint里面的x，则必不可能是最优解。
> ex : min $ \frac{1}{2} \left\| Ax-b \right\|^2 s.t.\left\| x \right\| _1 \leq S \iff min \left\{  \frac{1}{2} \left\| Ax-b \right\|^2 + \lambda \left\| x \right\| _1 \right\}  $
> 当s足够大（$ \rightarrow \infty  $）时，$ \lambda \rightarrow 0 $取小，反之s 取大时，$ \lambda  $取大，故二者等价。

> $$ 
min_{x \in R^d} h(x) = f(x)+g(x) 
$$
where f is a convex-$ \beta$-function,g is a convex function
> define 
$$ \begin{array}{r}m_t(x)=&(\nabla{f(x_t)},x-x_t)+\frac{\beta}{2} \left\| x-x_t \right\|^2 +g(x) \\ =& f(x_t)-\frac{1}{2\beta}\left\| \nabla{f(x_t)} \right\|^2 + \frac{\beta}{2} \left\| x-(x_t- \frac{1}{\beta }\nabla{f(x_t)}) \right\|^2  +g(x)
> \end{array} $$
> $ \therefore min_xm_t(x) \iff min_x \left\{ \frac{\beta}{2} \left\| x-(x_t- \frac{1}{\beta} \nabla{f(x_t)}) \right\|^2 + g(x) \right\}$
> $ \because h(x_{t+1}) \leq m_t(x_{t+1}) \leq m_t(x_t) = h(x_t) $
> $x_{t+1} = argmin_x \left\{ \frac{\beta}{2} \left\| x-(x_t- \frac{1}{\beta} \nabla{f(x_t)}) \right\|^2 + g(x) \right\} $是有道理的，使函数值减小

> 记
$$ x_{t+1} = prox_{\frac{1}{\gamma  }g}(x_t - \frac{1}{\beta}\nabla{f(x_t)}) = argmin_x \left\{ \frac{1}{2\gamma} \left\| x-(x_t- \frac{1}{\beta} \nabla{f(x_t)}) \right\|^2 + g(x) \right\}$$
$$ 
prox_{\gamma g} = argmin_x \left\{ \frac{1}{2 \gamma } \left\| x-z \right\|^2+g(x)  \right\} 
$$

ex1:
$ min_x \left\{ \frac{1}{2 \gamma } \left\| x-z \right\|^2  +\delta _{\Omega}(x)\right\} \iff min_{x \in \Omega }\left\{  \frac{1}{2\gamma} \left\| x-z \right\|^2  \right\}  $
$ \therefore prox_{\gamma \delta _{\Omega (x)}(z)} = argmin_{x \in \Omega } \left\{  \frac{1}{2\gamma} \left\| x-z \right\|^2   \right\}  = \Pi _\Omega(z) $

ex2:(lasso)
$ min \frac{1}{2} \left\| Ax-b \right\|^2 \quad s.t.\left\| x \right\| _1 \leq s \iff min_{x\in R^d} \left\{ \frac{1}{2} \left\| Ax-b \right\|^2 + \lambda \left\| x \right\|_1   \right\} \iff min_x \sum_{j=1}^{P}(\frac{1}{2}(x_j-z_j)^2+\lambda \left| x_i \right| ) $
等价于求解p个最小化问题
$ \Rightarrow f(x) = \frac{1}{2} \left\| Ax-b \right\|^2 g(x) = \lambda \left| x \right| \beta =\lambda _{\max}(A^TA) $

### subgradient

> **Definiton**: A subgradient of a convex possibble non-smooth function $ h: R^T \rightarrow R $ at point x is vector $ v \in R^p $ such that $ h(y) \geq h(x) + (v,y-x), \forall y \in dom(h) $; for differentiable function, $ f(y) \geq f(x)+(\nabla{f(x)},y-x)$

>  **Definiton**:subdifferential of h at point x is a set that containted all the subgradient of h at point x i.e.denote as $ \partial h(x) =${v|v is the subgradient of h at point x}

ex1:$ h(x) = \left| x \right|$ it's $\partial h(x)|_0 = [-1,1]$
ex2:$ h(x) = max(x,0) $ it's $ \partial h(x)|_0 = [0,1] $

> Lemma1: $ 0 \in \partial h(x) \iff$ x is a global minimum
> pf: if $ 0 \in \partial h(x) \Rightarrow h(y) \geq h(x)+(0,y-x) $

关于如何求解这个一维优化问题
$$ 
min_x \sum_{j=1}^{P}(\frac{1}{2}(x_j-z_j)^2+\lambda \left| x_i \right| )
$$
- when $ x_j>0 $,we have  $ \frac{\partial L(x_j)}{\partial x_j} = \beta (x_j-z_j)+\lambda =0 \Rightarrow x_j = z_j -\frac{\lambda}{\beta } $
- when $ x_j<0 $,we have  $ \frac{\partial L(x_j)}{\partial x_j} = \beta (x_j-z_j)-\lambda =0 \Rightarrow x_j = z_j +\frac{\lambda}{\beta } $
- when $ x_j = 0 $ ,$ 0 \in \partial L(0) \iff 0 \in \beta (x_j-z_j)+\lambda \partial \left| x_j \right| \iff \beta (z_j-x_j) \in \lambda \partial \left| x_j \right|  $，当 $ x_j = 0 $时，$ z_j \in [-\frac{\lambda}{\beta },\frac{\lambda}{\beta}] $

> **Definition**
$$
x_j^* = \left\{\begin{array}{l}
z_j - \frac{\lambda}{\beta}&,z_j>\frac{\lambda}{\beta }\\0&,z_j \in [-\frac{\lambda}{\beta},\frac{\lambda}{\beta}]\\z_j+\frac{\lambda}{\beta}&,z_j<-\frac{\lambda}{\beta}\\
\end{array}
\right.
$$
记 soft thresholding 为 $\qquad x_j^* = sgn(z)(\left| z \right| -\frac{\lambda}{\beta})_+  = S_{\frac{\lambda}{\beta}}(z_j)$
$$ 
sgn(z) = \left\{\begin{array}{l}
1&,z>0\\-1&,z<0\\0&,z=0\\    
\end{array}
\right.
$$

**因此要解决lasso 问题采用的梯度下降法为 $\qquad x_j^* = sgn(z)(\left| z \right| -\frac{\lambda}{\beta})_+ $ 其中 $ z_j = x_t - \frac{1}{\lambda_{\max}(A^TA}A^T(Ax_t-b) $ 即为 $ x^* = S_{\frac{\lambda}{\beta}}(x_t - \frac{1}{\lambda_{\max}(A^TA}A^T(Ax_t-b)) $**

### $ G_\gamma $ function

> Definition: $ G_{\gamma }(x) = \frac{1}{\gamma}(x-x^*) $, where $ x^* = prox_{\gamma g}(z) = argmin \left\{ \frac{1}{2\gamma} \left\| x-z \right\|^2 +g(x) \right\}  $ where $ z = x-\gamma \nabla{f(x)} $

> 若用迭代，则有 $ x^* = prox_{\gamma g} (x-\gamma \nabla{f(x)}) \Rightarrow x^* = x-\gamma \nabla{f(x)} \Rightarrow \nabla{f(x)} = \frac{1 }{\gamma }(x-x^*)$(当g为0不存在时)

> lemma1: $ G_{\gamma}(x) = 0 \iff 0 \in \partial h(x) $ furthermore we have that $ G_{\gamma}(x)-\nabla{f(x)}\in \partial g(x) $

pf: 
$$ 
x^+ = argmin_y \left\{ \frac{1}{2\gamma} \left\| y-(x-\gamma \nabla{f(x)}) \right\|^2 + g(y) \right\} 
$$
when it's minimum, we have 
$$ 
0 \in \partial L(x^+) = \frac{1}{\gamma}(x^+-(x-\gamma \nabla{f(x)}))+\partial g(x^+)\newline
\Rightarrow \frac{1}{\gamma}(x-x^+) \in \nabla{f(x)}+\partial g(x^+)
$$
if $ x = x^+ ,G_{\gamma}(x) = 0 \Rightarrow 0 \in \nabla{f(x^+)} + \partial g(x^+) = \partial L(x^+)\blacksquare  $

下面的一系列定理都非常重要(主定理的延伸)

> Theorem1: $ h(x) = f(x) + g(x) $,$ f(x) $ is a $ \alpha  $-strong convex, $ \beta  $-smooth function.
> g(x) $ is a convex function, 
> $ G_{\gamma }(x) = \frac{1}{\gamma}(x-x^+) $,
> where $ x^+ = argmin_y \left\{ \frac{1}{2\gamma}\left\| y-(x-\gamma \nabla{f(x)}) \right\|^2 +g(x) \right\}  $,
> then $ \forall y \in dom(h) $
> $ h(y) \geq h(x^+)+(G_{\gamma },y-x)+\frac{\alpha}{2} \left\| y-x \right\|^2 +\gamma (1-\frac{\gamma\beta}{2})\left\| G_{\gamma}(x) \right\|^2  $
> - $ \alpha  $ could be zero,$ h(y) \geq h(x^+) + (G_{\gamma },y-x)+ \gamma (1-\frac{\gamma\beta}{2})\left\| G_{\gamma}(x) \right\|^2$


> - $ \gamma = \frac{1}{\beta},x=y \Rightarrow h(x^+) \leq h(x) - \frac{1}{2\beta} \left\| G_{\gamma}(x) \right\|^2  $ 
> recall $ \beta  $-smooth function $ f(x_{t+1}) \leq f(x_t) - \frac{1}{2\beta} \left\| \nabla{f(x_t)} \right\|^2  $
> - $ \gamma = \frac{1}{\beta} ,y = x^* \newline \Rightarrow h^* = h(x^*) \geq h(x^+)+(G_{\gamma}(x),x^*-x)+\frac{\alpha}{2} \left\| x^*-x \right\|^2 +\frac{\gamma}{2} \left\| G_{\gamma}(x) \right\|^2  $
> - $ \gamma  = \frac{1}{\beta}, G_{\gamma} = \beta(x-x^+),\alpha = 0$
> $ h(y) \geq h(x^+)+ \beta (x-x^+,y-x)+\frac{\beta}{2}\left\| x-x^+ \right\|^2  $
> x与y互换则有
> $ h(x) \geq h(x^+)+ \beta (y-x^+,x-y)+\frac{\beta}{2}\left\| y-x^+ \right\|^2  $

pf:
$$ 
h(x^+) = f(x-\gamma G_{\gamma }(x)) = f(x-\gamma G_{\gamma }(x)) + g(x^+)
$$
by $ \beta  $ - smooth convex function lemma1,
$$ 
\leq f(x)-\gamma (\nabla{f(x)},G_{\gamma }(x))+\frac{\beta \gamma ^2}{2} \left\| G_{\gamma} \right\|^2 + g(x^+)
$$
by $ \alpha $ - strong convex function Definition
$$ 
\leq f(y) + (\nabla{f(x)},x-y) - \frac{\alpha}{2} \left\| x-y\right\|^2 - \gamma (\nabla{f(x)},G_{\gamma})+\frac{\beta\gamma^2}{2} \left\| G_{\gamma}(x) \right\|^2 + g(x^+)\newline
= f(y)+ (\nabla{f(x)},x^+-y) - \frac{\alpha}{2}\left\| x-y \right\|^2 + \frac{\beta \gamma^2}{2}\left\| G_{\gamma}(x) \right\|^2 + g(x^+)
$$
by $ G_{\gamma}(x) - \nabla{f(x)} \in g(x^+) $,we have 
$$ 
g(y) \geq g(x^+) + (G_{\gamma }(x) - \nabla{f(x)},y-x^+)
$$
$$ 
\therefore 上式 \leq f(y) + (\nabla{f(x)},x^+-y) - \frac{\alpha}{2} \left\| x-y \right\|^2 +\frac{\beta \gamma^2}{2} \left\| G_{\gamma}(x) \right\|^2 + g(y) + (G_{\gamma }(x)-\nabla{f(x)},x^+-y)\newline
\overset{\text{合并同类项}}{\leq}h(y)+(G_{\gamma},x^+-y)-\frac{\alpha}{2}\left\| x-y \right\|^2 + \frac{\beta \gamma^2}{2} \left\| G_{\gamma}(x) \right\|^2 \newline
\overset{x^+ = x- \gamma G_{\gamma }(x)}{=}h(y) + (G_{\gamma }(x),x-y) - \frac{\alpha}{2} \left\| x-y \right\|^2 + \gamma (\frac{\beta \gamma}{2}-1) \left\| G_{\gamma}(x) \right\|^2\blacksquare 
$$


> Theorem1: If h = f + g, f is $ \beta  $ -smooth ,convex g is convex 
> $$ 
h(x_T) - h(x^*) \leq \frac{\beta}{2T}\left\| x_1-x^* \right\|^2 
$$
in addition, if f is $ \alpha  $ - strong convex funciton ,we have 
$$ 
\left\| x_T - x^* \right\|^2 \leq exp(-\frac{\alpha}{\beta}T)\left\| x_-x^* \right\|^2 
$$

pf1:
$$ 
h(x^*) \geq h(x_{t+1})+(G_{\gamma }(x_t),x^*-x_t)+\frac{1}{2\beta}\left\| G_{\gamma}(x_t) \right\|^2 
$$where $ G_{\gamma }(x_t) = \beta (x_t-x_{t-1}) $
$$ 
h(x_{t+1}) - h^* \leq \frac{\beta}{2} \left\{ 2(x_t-x_{t-1},x^*-x_t)+ \left\| x_t-x_{t-1} \right\|^2 \right\} \newline
即 \leq \frac{\beta}{2} \left\{ \left\| x^*-x_{t-1}  \right\|^2 - \left\| x_t-x_{t-1} \right\|^2  \right\} \newline
\vdots 
$$
累加求平均
$$ 
h(x_{T}) - h^* \leq \frac{\beta}{2T} \left\| x_1-x^* \right\|^2 \blacksquare
$$
pf2:
$$ 
\left\| x_{t+1} - x^* \right\|^2  = \left\| x_t-\frac{1}{\beta}G_{\gamma}(x_t)-x^* \right\|^2 = \left\| x_t-x^* \right\|^2 + \frac{1}{\beta^2} \left\| G_{\gamma}(x_t) \right\|^2 -\frac{2}{\beta}(G_{\gamma}(x_t),x_t-x^*) 
$$
由推论第三条知:
$$ 
0 \geq h^* - h(x_{t+1}) \geq (G_{\gamma }(x_t),x^*-x_t)+ \frac{\alpha}{2} \left\| x^*-x_t \right\|^2+\frac{1}{2\beta}\left\| G_{\gamma}(x) \right\|^2\newline
\Rightarrow (G_{\gamma}(x_t),x_t-x^*) \geq \frac{\alpha}{2} \left\| x^*-x_t \right\|^2+\frac{1}{2\beta}\left\| G_{\gamma}(x) \right\|^2 $$

$$ 
\left\| x_{t+1}-x^* \right\|^2 \leq \left\| x_t-x^* \right\|^2 + \frac{1}{\beta^2} \left\| G_{\gamma}(x) \right\|^2 - \frac{2}{\beta}(\frac{1}{2\beta}\left\| G_{\gamma}(x) \right\|^2 + \frac{\alpha}{2} \left\| x_t-x^* \right\|^2 )\newline =(1-\frac{\alpha}{\beta}) \left\| x_t-x^* \right\|^2   
$$
$$ 
\Rightarrow \left\| x_T-x^* \right\|^2 \leq (1-\frac{\alpha}{\beta})^T \left\| x_1-x^* \right\|^2  \overset{T\rightarrow \infty}{=} exp(1-\frac{\alpha}{\beta}T) \left\| x_1-x^* \right\|^2 \blacksquare
$$

### Acclerated GD

> Theoerm1:(optional rate for $ \beta  $- smooth function)
> let T $ \leq \frac{P-1}{2}, \beta \geq 0 $ ,there exists a $ \beta  $-smooth convex quadratic f  such that 
> $$ 
min_{1 \leq t \leq T}(f(x_t)-f^*) \geq \frac{3 \beta \left\| x_0-x^* \right\|^2  }{32(1+T)^2}
$$


> $min_x h(x)$
> **AGD**: $ x_0,y_1 = x_0,a_0=1 $
> step1:$ x_t = y_t - \frac{1}{\beta} \nabla{h(y_t)} $
> step2: $ a_{t+1} = \frac{1+\sqrt{1+4a_t^2}}{2} $
> step3: $y_{t+1} = x_t + \frac{a_t-1}{a_{t+1}}(x_{t-1}-x_t)$

more general: **A proximal GD**
> $ min \left\{ h(x) = f(x)+g(x) \right\}  $ where f is a convex $ \beta  $ - smooth function and g is a convex function
> step1: $ x_t = Prox_{g/\beta}(y_t-\frac{1}{\beta}\nabla{y_t}) $ 若prox 为 soft thresholding 称之为 FISTA
> step2:  $ a_{t+1} = \frac{1+\sqrt{1+4a_t^2}}{2} $
> step3: $y_{t+1} = x_t + \frac{a_t-1}{a_{t+1}}(x_{t-1}-x_t)$

> lemma1: 
> - $ \left\{ a_t \right\}  $ 单调递增
> - $ a_t > \frac{t+1}{2} $

pf: we know $ a_{t+1}^2 -a_{t+1} = a_t^2 $ and $ a_0 = 1 $
$ a_0 > \frac{t+1}{2} = \frac{1}{2} $
suppose $ a_t > \frac{t+1}{2} $
$ a_{t+1}^2 - a_{t+1}-a_{t}^2 = 0 \Rightarrow a_{t+1}^2 - a_{t+1} > \frac{(1+t)^2}{4} \Rightarrow a_{t+1} \geq \frac{1+\sqrt{1+(1+t)^2}}{2} \geq \frac{1+1+t}{2} = \frac{t+2}{2} $

> Theorem1: let $ \left\{ (x_t,y_t) \right\}_{t=1}^T  $ be generated APGD, then for any $ T \geq 1 $ 
> $$ 
h(x_T) - h^* \leq 2 \beta \left\| x_0-x^* \right\|^2/(1+T)^2 
$$

ex:

$$ 
min \sum_{i=1}^N log(1+exp(a_i^Tx)) - b^TAx\newline
s.t. \left\| x \right\| _1 \leq s
$$
$$ 
\iff min \left\{ \sum_{i=1}^N log(1+exp(a_i^Tx)) - b^TAx+\lambda \left\| x \right\| _1 \right\} 
$$

> lemma2
> - $ h(x) \geq h(x^+)+\beta (y-x^+,x-y)+\frac{\beta}{2} \left\| y-x^+ \right\|^2  $ ,$ G_{\gamma} $function theorem 1推论三
> - $ a_{t+1}^2 - a_{t+1}  = a_t^2 $
> - $ a_t \geq \frac{t+1}{2}$

> lemma3: $ \left\{ (x_t,y_t) \right\} $generated by AGD with constant step size $\frac{1}{\beta}$, then for every $ t \geq 1 $,we have 
$$ 
a_t^2v_t - a_{t+1}^2v_{t+1} \geq \frac{\beta}{2} (\left\| u_{t+1} \right\|^2 -\left\| u_t \right\|^2 )
$$
where $ v_t = h(x_t)-h^* $ and $ u_t = a_tx_t-(a_t-1)x_{t-1}-x^* $

pf:
in $ (1) $ let $ x=x_t,y=y_{t+1} \Rightarrow x^+ = x_{t+1} $
$$ 
h(x_t) \geq h(x_{t+1})+\beta (y_{t+1}-x_{t+1},x_t-y_{t+1})+\frac{\beta}{2}\left\| y_{t+1}-x_{t+1} \right\|^2 \newline
h(x_t)-h^* \geq h(x_{t+1})-h^*+\beta (y_{t+1}-x_{t+1},x_t-y_{t+1})+\frac{\beta}{2}\left\| y_{t+1}-x_{t+1} \right\|^2 \newline
\Rightarrow v_t \geq v_{t+1} + \beta (y_{t+1}-x_{t+1},x_t-y_{t+1})+\frac{\beta}{2}\left\| y_{t+1}-x_{t+1} \right\|^2 \newline
\Rightarrow \frac{2}{\beta}(v_t-v_{t+1})\geq 2 (y_{t+1}-x_{t+1},x_t-y_{t+1})+\left\| y_{t+1}-x_{t+1} \right\|^2 \tag{2}
$$
let $ x = x^*,y = y_{t+1} \Rightarrow x^+ = x_{t+1} $
$$ 
h^* \geq h(x_{t+1}) + \beta(y_{t+1}-x_{t+1},x^*-y_{t+1}) + \frac{\beta}{2} \left\| y_{t+1}-x_{t+1} \right\|^2 \newline
h^* - h(x_{t+1}) \geq \beta(y_{t+1}-x_{t+1},x^*-y_{t+1}) + \frac{\beta}{2} \left\| y_{t+1}-x_{t+1} \right\|^2 \newline
\Rightarrow -\frac{2}{\beta}v_{t+1} \geq 2(y_{t+1}-x_{t+1},x^*-y_{t+1}) + \left\| x_{t+1}-y_{t+1} \right\|^2 \tag{3}
$$

$$ 
(2)\times (a_t-1) + (3) \Rightarrow (4)\newline
\frac{2}{\beta}\left\{ (a_{t+1}-1)v_t-a_{t+1}v_{t+1} \right\} \geq a_{t+1} \left\| x_{t+1}-y_{t+1} \right\|^2 + 2(x_{t+1}-y_{t+1},a_ty_{t+1}-(a_{t+1}-1)x_t-x^*)\tag{4}
$$ 

$$
(4) \times a_{t+1}\newline
\frac{2}{\beta}[\underbrace{(a_{t+1}^2-a_{t+1}}_{a_{t}^2})v_t-a_{t+1}^2v_{t+1}] \geq \ldots \blacksquare
$$

> lemma4:let $ \left\{ c_t,b_t \right\}  $ be positive real numbers, that satisfy $$ 
c_t - c_{t+1} \geq b_{t+1}-b_{t}, \forall t \geq 1
$$
and 
$$ 
c_1 + b_1 \leq c (where\quad c\quad is \quad constant)
$$
then
$$ 
c_t \leq c \Leftarrow \left\{ c_t+b_t \leq c \right\} 数学归纳法自己证
$$

recall theorem1 is 

> Theorem1: let $ \left\{ (x_t,y_t) \right\}_{t=1}^T  $ be generated APGD, then for any $ T \geq 1 $ 
> $$ 
h(x_T) - h^* \leq 2 \beta \left\| x_0-x^* \right\|^2/(1+T)^2 
$$
now we prove it by these lemma.

pf:
let $ c_t = a_t^2v_t ,\quad b_t = \frac{\beta}{2} \left\| u_t\right\|^2 \Rightarrow c_t-c_{t+1} \geq b_{t+1}-b_t $
$$ 
\Rightarrow c_t \leq c ,\quad c_1+b_1 \leq c \newline c_1 = a_1^2v_1 = h(x_1)-h^*\newline
b_1 = \frac{\beta}{2} \left\| u_1 \right\|^2  = \frac{\beta}{2}\left\| x_1 - x^* \right\|^2 
$$
let $ x= x^*,y=y_1 \Rightarrow x^+ = x_1 $ 
$$ 
h^* \geq h(x_1)+\beta(y_1-x_1,x^*-y_1)+\frac{\beta}{2}\left\| y_1-x_1 \right\|^2 \newline
-c_1 \geq \frac{\beta}{2}[2(y_1-x_1,x^*-y_1)+ \left\| x_1-y_1 \right\|^2  ]\newline
\overset{by 2(a,b) = \left\| a+b \right\|^2 -\left\| a \right\|^2 -\left\| b \right\|^2 }{=}-c_1 \geq \frac{\beta}{2}(\left\| x_1-x^* \right\|^2 -\left\| x^*-y_1 \right\|^2 ) \newline
 = b_1-\frac{\beta}{2} \left\| x^*-y_1 \right\|^2 \newline
 b_1+c_1 \leq \frac{\beta}{2}\left\| x^*-y_1 \right\|^2  = \frac{\beta}{2}\left\| x^*-x_0 \right\|^2
$$
$$ 
\because c_t \leq c \newline
\therefore a_t^2v_t \leq \frac{\beta}{2} \left\| x^*-x_0 \right\|^2 \Rightarrow v_t \leq \frac{\beta \left\| x^*-x_0 \right\|^2  }{2a_t^2}\newline
\overset{by \quad a_t \geq \frac{t+1}{2}}{\Rightarrow }h(x_T)-h^* \leq \ldots \blacksquare
$$

### ADMM(Alternateive Direction Method of Multiplies)